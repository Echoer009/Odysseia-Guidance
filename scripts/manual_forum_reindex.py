# -*- coding: utf-8 -*-

import asyncio
import logging
import os
import sys
import shutil
import discord
import argparse
import json
import time  # æ–°å¢ï¼šç”¨äºå†·å´
from tqdm import tqdm
from tqdm.asyncio import tqdm_asyncio

# ... (å¯¼å…¥è·¯å¾„ä¿æŒä¸å˜) ...
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src import config as main_config

# ... (æ—¥å¿—é…ç½®ä¿æŒä¸å˜) ...
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(name)s] - %(message)s",
    stream=sys.stdout,
)
log = logging.getLogger(__name__)

DB_DIR = os.path.join(main_config.DATA_DIR, "forum_chroma_db")
DB_STATUS_FILE = os.path.join(main_config.DATA_DIR, "forum_sync_status.db")


# ... (clear_existing_database å‡½æ•°ä¿æŒä¸å˜) ...
def clear_existing_database():
    log.info("å¼€å§‹æ¸…ç©ºæ—§çš„è®ºå›ç´¢å¼•æ•°æ®åº“...")
    try:
        if os.path.exists(DB_DIR):
            shutil.rmtree(DB_DIR)
            log.info(f"æˆåŠŸåˆ é™¤ç›®å½•: {DB_DIR}")

        if os.path.exists(DB_STATUS_FILE):
            os.remove(DB_STATUS_FILE)
            log.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶: {DB_STATUS_FILE}")

        return True
    except Exception as e:
        log.error(f"æ¸…ç†æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return False


async def restore_from_jsonl(jsonl_file: str):
    """
    ã€ä¼˜åŒ–ç‰ˆã€‘ä» JSONL æ–‡ä»¶æ¢å¤ç´¢å¼•ã€‚
    ç‰¹ç‚¹ï¼šå†…å­˜å ç”¨ä½ï¼ŒåŒ…å« API é€Ÿç‡é™åˆ¶ä¿æŠ¤ã€‚
    """
    log.info(f"ğŸ”¥ å¼€å§‹ä»å¤‡ä»½æ–‡ä»¶ '{jsonl_file}' æ¢å¤ç´¢å¼•...")

    # 1. å¼ºåˆ¶æ¸…ç†æ•°æ®åº“
    if not clear_existing_database():
        return

    if not os.path.exists(jsonl_file):
        log.error(f"é”™è¯¯: å¤‡ä»½æ–‡ä»¶ '{jsonl_file}' ä¸å­˜åœ¨ã€‚")
        return

    # 2. å»¶è¿Ÿå¯¼å…¥æœåŠ¡ï¼Œç¡®ä¿åœ¨æ•°æ®åº“æ¸…ç†åæ‰§è¡Œ
    from src.chat.features.forum_search.services.forum_search_service import (
        forum_search_service,
    )

    try:
        if hasattr(forum_search_service, "init_async"):
            await forum_search_service.init_async()
        log.info("æœç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆã€‚")
    except Exception as e:
        log.error(f"æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

    # 3. æµå¼è¯»å– + æ‰¹é‡å¤„ç†
    batch_size = 20  # â¬‡ï¸ è°ƒå°ä¸€ç‚¹ï¼Œé˜²æ­¢ Gemini 429 é”™è¯¯
    current_batch = []

    try:
        # å…ˆæ‰«ä¸€éè·å–æ€»è¡Œæ•°ç”¨äºè¿›åº¦æ¡ (è¿™ä¸€æ­¥å¾ˆå¿«)
        with open(jsonl_file, "r", encoding="utf-8") as f:
            total_lines = sum(1 for _ in f)

        log.info(f"å…±æ‰¾åˆ° {total_lines} æ¡è®°å½•ï¼Œå‡†å¤‡å¼€å§‹...")

        with open(jsonl_file, "r", encoding="utf-8") as f:
            pbar = tqdm(total=total_lines, desc="æ¢å¤è¿›åº¦")

            for line in f:
                line = line.strip()
                if not line:
                    continue

                try:
                    item = json.loads(line)
                    current_batch.append(item)
                except json.JSONDecodeError:
                    continue

                # å‡‘å¤Ÿä¸€æ‰¹ï¼Œå‘é€å¤„ç†
                if len(current_batch) >= batch_size:
                    await process_batch(current_batch)
                    pbar.update(len(current_batch))
                    current_batch = []

                    # ğŸ›Œ å…³é”®ï¼šä¼‘æ¯ä¸€ä¸‹ï¼
                    # æ—¢ä¸ºäº† Gemini ä¸æŠ¥ 429ï¼Œä¹Ÿä¸ºäº†ç¡¬ç›˜ I/O èƒ½å–˜å£æ°”
                    # å¦‚æœä½ çš„ VPS å¾ˆå¡ï¼Œå»ºè®®æ”¹ä¸º 2.0 æˆ– 3.0
                    await asyncio.sleep(1.5)

            # å¤„ç†å‰©ä½™çš„å°¾å·´
            if current_batch:
                await process_batch(current_batch)
                pbar.update(len(current_batch))

            pbar.close()

        log.info(f"ğŸ‰ æ¢å¤å®Œæˆï¼æ‰€æœ‰ {total_lines} æ¡è®°å½•å·²é‡æ–°å¤„ç†ã€‚")

    except Exception as e:
        log.error(f"æ¢å¤è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)


async def process_batch(batch_items):
    """è¾…åŠ©å‡½æ•°ï¼šå¤„ç†ä¸€ä¸ªæ‰¹æ¬¡"""
    ids = [item["id"] for item in batch_items if item.get("id")]
    documents = [item["document"] for item in batch_items if item.get("document")]
    metadatas = [item.get("metadata", {}) for item in batch_items]

    if not ids or not documents:
        return

    # å»¶è¿Ÿå¯¼å…¥æœåŠ¡
    from src.chat.features.forum_search.services.forum_search_service import (
        forum_search_service,
    )

    try:
        await forum_search_service.add_documents_batch(
            ids=ids, documents=documents, metadatas=metadatas
        )
    except Exception as e:
        log.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")


async def reindex_forums(rebuild: bool, restore_from: str = None):
    """è¿æ¥åˆ°Discordå¹¶æ‰§è¡Œé‡æ–°ç´¢å¼•ä»»åŠ¡ï¼Œæˆ–ä»å¤‡ä»½æ¢å¤ã€‚"""
    if restore_from:
        await restore_from_jsonl(restore_from)
        return

    # --- ä»¥ä¸‹æ˜¯åŸå§‹çš„ä» Discord æŠ“å–é€»è¾‘ ---
    intents = discord.Intents.default()
    intents.guilds = True
    intents.message_content = True
    intents.members = True
    client = discord.Client(intents=intents)

    @client.event
    async def on_ready():
        log.info(f"æœºå™¨äººå·²ä½œä¸º {client.user} ç™»å½•ï¼Œå‡†å¤‡å¼€å§‹ç´¢å¼•ã€‚")

        if rebuild:
            if not clear_existing_database():
                log.error("æ•°æ®åº“æ¸…ç†å¤±è´¥ï¼Œç´¢å¼•ä»»åŠ¡å·²ä¸­æ­¢ã€‚")
                await client.close()
                return
        else:
            log.info("å°†æ‰§è¡Œæ›´æ–°/å¢é‡ç´¢å¼•ï¼ˆè·³è¿‡æ•°æ®åº“æ¸…ç†ï¼‰ã€‚")

        # å»¶è¿Ÿå¯¼å…¥ï¼Œç¡®ä¿æœåŠ¡åœ¨éœ€è¦æ—¶æ‰åˆå§‹åŒ–
        from src.chat.config import chat_config
        from src.chat.features.forum_search.services.forum_search_service import (
            forum_search_service,
        )

        channel_ids = chat_config.FORUM_SEARCH_CHANNEL_IDS
        if not channel_ids:
            log.warning("æ²¡æœ‰åœ¨é…ç½®ä¸­æ‰¾åˆ°ä»»ä½•è®ºå›é¢‘é“IDã€‚")
            await client.close()
            return

        log.info(f"å°†è¦å¤„ç†çš„é¢‘é“ID: {channel_ids}")

        for channel_id in channel_ids:
            channel = client.get_channel(channel_id)
            if not isinstance(channel, discord.ForumChannel):
                log.warning(f"ID {channel_id} ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„è®ºå›é¢‘é“ï¼Œå·²è·³è¿‡ã€‚")
                continue

            log.info(f"--- å¼€å§‹å¤„ç†é¢‘é“: {channel.name} ({channel.id}) ---")
            try:
                active_threads = channel.threads
                archived_threads_iterator = channel.archived_threads(limit=100)
                archived_threads = [t async for t in archived_threads_iterator]

                all_threads_dict = {t.id: t for t in active_threads}
                all_threads_dict.update({t.id: t for t in archived_threads})

                sorted_threads = sorted(
                    all_threads_dict.values(),
                    key=lambda t: t.created_at,
                    reverse=True,
                )
                threads_to_process = sorted_threads[:100]
                log.info(f"æ‰¾åˆ° {len(threads_to_process)} ä¸ªå¸–å­å‡†å¤‡å¤„ç†ã€‚")

                semaphore = asyncio.Semaphore(chat_config.FORUM_POLL_CONCURRENCY)
                tasks = []

                async def process_with_semaphore(thread):
                    async with semaphore:
                        await forum_search_service.process_thread(thread)

                for thread in threads_to_process:
                    tasks.append(process_with_semaphore(thread))

                for f in tqdm_asyncio.as_completed(
                    tasks, desc=f"ç´¢å¼•é¢‘é“ {channel.name}"
                ):
                    await f

            except Exception as e:
                log.error(f"å¤„ç†é¢‘é“ {channel.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

        log.info("æ‰€æœ‰é¢‘é“çš„ç´¢å¼•ä»»åŠ¡å·²å®Œæˆã€‚æœºå™¨äººå°†è‡ªåŠ¨å…³é—­ã€‚")
        await client.close()

    try:
        token = os.getenv("DISCORD_TOKEN")
        if not token:
            log.critical("é”™è¯¯: DISCORD_TOKEN æœªåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ï¼")
            return
        await client.start(token)
    except discord.LoginFailure:
        log.error("æœºå™¨äººä»¤ç‰Œæ— æ•ˆï¼Œè¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶é…ç½®ã€‚")
    except Exception as e:
        log.error(f"å¯åŠ¨æœºå™¨äººæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)


async def main():
    parser = argparse.ArgumentParser(description="æ‰‹åŠ¨é‡æ–°ç´¢å¼•Discordè®ºå›å¸–å­ã€‚")
    parser.add_argument(
        "--rebuild",
        action="store_true",
        help="å¦‚æœè®¾ç½®æ­¤æ ‡å¿—ï¼Œå°†å®Œå…¨æ¸…ç©ºå¹¶é‡å»ºç´¢å¼•æ•°æ®åº“ã€‚å¦åˆ™ï¼Œå°†æ‰§è¡Œæ›´æ–°/å¢é‡ç´¢å¼•ã€‚",
    )
    parser.add_argument(
        "--restore-from",
        type=str,
        default=None,
        help="æä¾›ä¸€ä¸ª JSONL æ–‡ä»¶çš„è·¯å¾„ï¼Œå°†ä»è¯¥æ–‡ä»¶æ¢å¤ç´¢å¼•ï¼Œè€Œä¸æ˜¯ä» Discord æŠ“å–ã€‚",
    )
    args = parser.parse_args()

    # å¦‚æœæä¾›äº† restore_fromï¼Œåˆ™ rebuild æ ‡å¿—è‡ªåŠ¨ä¸º Trueï¼Œå› ä¸ºæ¢å¤æ€»æ˜¯éœ€è¦ä¸€ä¸ªå¹²å‡€çš„ç¯å¢ƒ
    should_rebuild = args.rebuild or args.restore_from is not None

    await reindex_forums(rebuild=should_rebuild, restore_from=args.restore_from)


if __name__ == "__main__":
    from dotenv import load_dotenv

    # ç¡®ä¿ .env æ–‡ä»¶å·²åŠ è½½
    load_dotenv()
    asyncio.run(main())
